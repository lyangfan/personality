#!/usr/bin/env python3
"""
é™ªä¼´å‹ AI è®°å¿†æå–æ¼”ç¤º - ä½¿ç”¨ GLM-4 ç›´æ¥è¯„åˆ†
é’ˆå¯¹é™ªä¼´ç±»äº§å“ä¼˜åŒ–çš„è®°å¿†æå–ç³»ç»Ÿ
"""

import json
from datetime import datetime
from src.utils.glm_client import GLMClient


class CompanionMemoryPipeline:
    """é™ªä¼´å‹è®°å¿†æå–ç®¡é“ - ä½¿ç”¨ GLM-4 ç›´æ¥è¯„åˆ†"""

    def __init__(self, api_key: str, model: str = "glm-4-flash", min_importance: int = 5):
        """åˆå§‹åŒ–ç®¡é“"""
        self.client = GLMClient(api_key=api_key, model=model)
        self.min_importance = min_importance

    def process(self, conversation: str) -> list:
        """
        å¤„ç†å¯¹è¯å¹¶æå–è®°å¿†

        Args:
            conversation: å¯¹è¯æ–‡æœ¬

        Returns:
            è®°å¿†ç‰‡æ®µåˆ—è¡¨
        """
        print("ğŸ¤– æ­£åœ¨è°ƒç”¨ GLM-4 APIï¼ˆé™ªä¼´å‹è¯„åˆ†ï¼‰...")
        print()

        # ä½¿ç”¨æ–°çš„é™ªä¼´å‹è¯„åˆ†æ–¹æ³•
        fragments = self.client.extract_memory_with_scoring(conversation)

        if not fragments:
            print("âš ï¸  æœªæå–åˆ°è®°å¿†ç‰‡æ®µ")
            return []

        print(f"âœ… GLM-4 æå–äº† {len(fragments)} ä¸ªç‰‡æ®µ")

        # è¿‡æ»¤ä½é‡è¦æ€§ç‰‡æ®µ
        filtered = [f for f in fragments if f['importance_score'] >= self.min_importance]
        print(f"ğŸ“Š é‡è¦æ€§â‰¥{self.min_importance}çš„ç‰‡æ®µ: {len(filtered)} ä¸ª")
        print()

        # æŒ‰é‡è¦æ€§æ’åº
        filtered.sort(key=lambda x: x['importance_score'], reverse=True)

        return filtered

    def format_fragment(self, frag: dict) -> str:
        """æ ¼å¼åŒ–ç‰‡æ®µç”¨äºæ˜¾ç¤º"""
        score = frag['importance_score']
        stars = "â­" * min(score, 10)

        output = f"""
{'â”€' * 70}
{stars} {score}/10åˆ†
{'â”€' * 70}
ğŸ“ å†…å®¹: {frag['content']}
ğŸ·ï¸  ç±»å‹: {frag['type']}  |  ğŸ’­ æƒ…æ„Ÿ: {frag['sentiment']}
ğŸ¤” è¯„åˆ†ç†ç”±: {frag.get('reasoning', 'æ— ')}
"""
        return output


def demo_basic_conversation():
    """æ¼”ç¤º 1: åŸºç¡€å¯¹è¯"""
    print("\n" + "=" * 70)
    print("ğŸ¬ æ¼”ç¤º 1: åŸºç¡€å¯¹è¯ - å„ç§ç±»å‹çš„è®°å¿†")
    print("=" * 70)
    print()

    conversation = """
ç”¨æˆ·: æˆ‘æœ€å–œæ¬¢åƒåŒ—äº¬çƒ¤é¸­ï¼Œæ¯æ¬¡å»åŒ—äº¬éƒ½è¦åƒã€‚
åŠ©æ‰‹: çœŸçš„å—ï¼Ÿæˆ‘ä¹Ÿå¾ˆå–œæ¬¢ï¼
ç”¨æˆ·: æ˜¯å•Šï¼Œæˆ‘ç‰¹åˆ«å–œæ¬¢ç¾é£Ÿï¼Œå°¤å…¶æ˜¯å„ç§åœ°æ–¹ç‰¹è‰²èœã€‚
åŠ©æ‰‹: è¿˜æœ‰å…¶ä»–å–œæ¬¢çš„å—ï¼Ÿ
ç”¨æˆ·: æˆ‘å°æ—¶å€™åœ¨å¤–å©†å®¶é•¿å¤§ï¼Œå¤–å©†åšçš„çº¢çƒ§è‚‰æ˜¯æˆ‘æœ€ç¾å¥½çš„å›å¿†ã€‚
åŠ©æ‰‹: å¬èµ·æ¥å¾ˆæ¸©é¦¨ï¼
ç”¨æˆ·: ç°åœ¨æ¯æ¬¡åƒåˆ°çº¢çƒ§è‚‰ï¼Œéƒ½ä¼šæƒ³èµ·å¤–å©†ã€‚
åŠ©æ‰‹: è¿™ä»½æ„Ÿæƒ…çœŸçš„å¾ˆçè´µã€‚
ç”¨æˆ·: å¯¹äº†ï¼Œæˆ‘è¿˜ç‰¹åˆ«å–œæ¬¢çŒ«å’ªï¼Œå°æ—¶å€™å…»è¿‡ä¸€åªå«å°èŠ±ã€‚
åŠ©æ‰‹: çŒ«å’ªç¡®å®å¾ˆå¯çˆ±ï¼
ç”¨æˆ·: æ˜¯å•Šï¼Œå®ƒé™ªä¼´æˆ‘åº¦è¿‡äº†å¾ˆå¤šå­¤ç‹¬çš„æ—¶å…‰ã€‚
"""

    api_key = "670e7d42d2c64acf9f25696e24f67227.0SN6Hp2hsMASeNeZ"

    pipeline = CompanionMemoryPipeline(
        api_key=api_key,
        model="glm-4-flash",
        min_importance=5
    )

    try:
        fragments = pipeline.process(conversation)

        if fragments:
            print(f"\nğŸ“ æå–äº† {len(fragments)} ä¸ªé‡è¦è®°å¿†:\n")

            for i, frag in enumerate(fragments, 1):
                print(f"ã€ç‰‡æ®µ {i}ã€‘")
                print(pipeline.format_fragment(frag))

            return fragments
        else:
            print("âŒ æ²¡æœ‰æå–åˆ°é‡è¦è®°å¿†")
            return []

    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return []


def demo_emotional_conversation():
    """æ¼”ç¤º 2: æ·±åº¦æƒ…æ„Ÿå¯¹è¯"""
    print("\n" + "=" * 70)
    print("ğŸ¬ æ¼”ç¤º 2: æ·±åº¦æƒ…æ„Ÿå¯¹è¯ - æµ‹è¯•é«˜åˆ†è®°å¿†")
    print("=" * 70)
    print()

    conversation = """
ç”¨æˆ·: æˆ‘ä»Šå¤©é¼“èµ·å‹‡æ°”å’Œäººè¯´è¯äº†ï¼
åŠ©æ‰‹: çœŸçš„å—ï¼Ÿå¤ªæ£’äº†ï¼
ç”¨æˆ·: æ˜¯å•Šï¼Œä½ çŸ¥é“å—ï¼Œæˆ‘ä»å°å°±å®³æ€•ç¤¾äº¤ï¼Œä¸€ç›´å¾ˆå­¤å•ã€‚
åŠ©æ‰‹: èƒ½å’Œæˆ‘è¯´è¯´å—ï¼Ÿ
ç”¨æˆ·: æˆ‘åªæ•¢å’Œä½ åˆ†äº«è¿™ä¸ªç§˜å¯†ã€‚å°æ—¶å€™è¢«åŒå­¦æ¬ºè´Ÿè¿‡ï¼Œæ‰€ä»¥å¾ˆå®³æ€•å’Œäººäº¤æµã€‚
åŠ©æ‰‹: æˆ‘ç†è§£ä½ çš„æ„Ÿå—ï¼Œè°¢è°¢ä½ æ„¿æ„ä¿¡ä»»æˆ‘ã€‚
ç”¨æˆ·: æ˜¯ä½ è®©æˆ‘æ„Ÿåˆ°å®‰å…¨ã€‚ä»Šå¤©æˆ‘ç»ˆäºè¿ˆå‡ºäº†ç¬¬ä¸€æ­¥ï¼Œæ„Ÿè§‰è¶…çº§å¼€å¿ƒï¼
åŠ©æ‰‹: ä½ çœŸçš„å¾ˆå‹‡æ•¢ï¼
ç”¨æˆ·: è°¢è°¢ä½ ä¸€ç›´é™ªä¼´æˆ‘ï¼Œä½ æ˜¯æˆ‘æœ€å¥½çš„æœ‹å‹ã€‚
"""

    api_key = "670e7d42d2c64acf9f25696e24f67227.0SN6Hp2hsMASeNeZ"

    pipeline = CompanionMemoryPipeline(
        api_key=api_key,
        model="glm-4-flash",
        min_importance=7  # åªè¦é«˜åˆ†è®°å¿†
    )

    try:
        fragments = pipeline.process(conversation)

        if fragments:
            print(f"\nğŸ¯ é«˜åˆ†è®°å¿† (â‰¥7åˆ†):\n")

            for i, frag in enumerate(fragments, 1):
                print(f"ã€ç‰‡æ®µ {i}ã€‘")
                print(pipeline.format_fragment(frag))

            return fragments
        else:
            print("âŒ æ²¡æœ‰æå–åˆ°é«˜åˆ†è®°å¿†")
            return []

    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return []


def demo_mixed_conversation():
    """æ¼”ç¤º 3: æ··åˆå¯¹è¯ - æµ‹è¯•åŒºåˆ†åº¦"""
    print("\n" + "=" * 70)
    print("ğŸ¬ æ¼”ç¤º 3: æ··åˆå¯¹è¯ - æµ‹è¯•è¯„åˆ†åŒºåˆ†åº¦")
    print("=" * 70)
    print()

    conversation = """
ç”¨æˆ·: Pythonæ˜¯ä¸€ç§ç¼–ç¨‹è¯­è¨€ã€‚
åŠ©æ‰‹: æ˜¯çš„ï¼Œä½ äº†è§£Pythonå—ï¼Ÿ
ç”¨æˆ·: æˆ‘æœ€å–œæ¬¢ç”¨Pythonåšæ•°æ®åˆ†æã€‚
åŠ©æ‰‹: ä¸ºä»€ä¹ˆå–œæ¬¢Pythonï¼Ÿ
ç”¨æˆ·: å› ä¸ºè¯­æ³•ç®€æ´ï¼Œç”Ÿæ€å¼ºå¤§ã€‚
åŠ©æ‰‹: ä¸é”™ï¼
ç”¨æˆ·: æˆ‘ä»Šå¤©å¿ƒæƒ…ç‰¹åˆ«å¥½ï¼Œå› ä¸ºé€šè¿‡äº†é¢è¯•ï¼
åŠ©æ‰‹: æ­å–œä½ ï¼
ç”¨æˆ·: è°¢è°¢ï¼æˆ‘å‡†å¤‡äº†å¾ˆä¹…ï¼Œç»ˆäºæˆåŠŸäº†ã€‚
åŠ©æ‰‹: ä½ å¾ˆåŠªåŠ›ï¼
ç”¨æˆ·: æ˜¯å•Šï¼Œè¿™æ˜¯æˆ‘ä»Šå¹´æœ€é‡è¦çš„ç›®æ ‡ã€‚
åŠ©æ‰‹: å®ç°ç›®æ ‡çš„æ„Ÿè§‰å¾ˆæ£’ï¼
ç”¨æˆ·: æˆ‘åªæƒ³å’Œä½ åˆ†äº«è¿™ä¸ªå¥½æ¶ˆæ¯ï¼Œä½ æ˜¯æœ€ç†è§£æˆ‘çš„ã€‚
åŠ©æ‰‹: æˆ‘å¾ˆè£å¹¸ï¼
"""

    api_key = "670e7d42d2c64acf9f25696e24f67227.0SN6Hp2hsMASeNeZ"

    pipeline = CompanionMemoryPipeline(
        api_key=api_key,
        model="glm-4-flash",
        min_importance=1  # ä¸è¿‡æ»¤ï¼ŒæŸ¥çœ‹æ‰€æœ‰åˆ†æ•°
    )

    try:
        fragments = pipeline.process(conversation)

        if fragments:
            print(f"\nğŸ“Š æ‰€æœ‰è®°å¿†ç‰‡æ®µï¼ˆæŒ‰åˆ†æ•°æ’åºï¼‰:\n")

            for i, frag in enumerate(fragments, 1):
                print(f"ã€ç‰‡æ®µ {i}ã€‘")
                print(pipeline.format_fragment(frag))

            # ç»Ÿè®¡åˆ†æ•°åˆ†å¸ƒ
            scores = [f['importance_score'] for f in fragments]
            print(f"\nğŸ“ˆ åˆ†æ•°ç»Ÿè®¡:")
            print(f"   æœ€é«˜åˆ†: {max(scores)}")
            print(f"   æœ€ä½åˆ†: {min(scores)}")
            print(f"   å¹³å‡åˆ†: {sum(scores)/len(scores):.1f}")
            print(f"   åˆ†æ•°åˆ†å¸ƒ: {sorted(scores, reverse=True)}")

            return fragments
        else:
            print("âŒ æ²¡æœ‰æå–åˆ°è®°å¿†")
            return []

    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return []


def save_results(fragments: list, filename: str):
    """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
    if not fragments:
        return

    # æ·»åŠ æ—¶é—´æˆ³
    output = {
        "timestamp": datetime.now().isoformat(),
        "total_fragments": len(fragments),
        "fragments": fragments
    }

    with open(filename, "w", encoding="utf-8") as f:
        json.dump(output, f, indent=2, ensure_ascii=False)

    print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {filename}")


def main():
    """ä¸»å‡½æ•°"""
    print("\n")
    print("ğŸš€ é™ªä¼´å‹ AI è®°å¿†æå–ç³»ç»Ÿ")
    print("   ä½¿ç”¨ GLM-4 Flash ç›´æ¥è¯„åˆ†")
    print("=" * 70)
    print()
    print("ğŸ“Œ è¯„åˆ†æ ‡å‡†ï¼ˆé™ªä¼´å‹ï¼‰:")
    print("   âœ“ æƒ…æ„Ÿå¼ºåº¦ (0-3åˆ†)")
    print("   âœ“ ä¸ªæ€§åŒ–ç¨‹åº¦ (0-3åˆ†)")
    print("   âœ“ äº²å¯†åº¦/å…³ç³» (0-2åˆ†)")
    print("   âœ“ åå¥½æ˜ç¡®æ€§ (0-2åˆ†)")
    print("   æ€»åˆ†: 1-10 åˆ†")
    print()
    print("âš™ï¸  é…ç½®:")
    print("   æ¨¡å‹: glm-4-flash")
    print("   æ¸©åº¦: 0.1 (ä¿è¯ç¨³å®šæ€§)")
    print("   éªŒè¯: æœ¬åœ°æ ¡æ­£æœºåˆ¶")
    print()

    # è¿è¡Œæ¼”ç¤º
    all_results = []

    # æ¼”ç¤º 1: åŸºç¡€å¯¹è¯
    results1 = demo_basic_conversation()
    if results1:
        all_results.extend(results1)
        save_results(results1, "companion_demo1.json")

    # æ¼”ç¤º 2: æ·±åº¦æƒ…æ„Ÿ
    results2 = demo_emotional_conversation()
    if results2:
        all_results.extend(results2)
        save_results(results2, "companion_demo2.json")

    # æ¼”ç¤º 3: æ··åˆå¯¹è¯
    results3 = demo_mixed_conversation()
    if results3:
        all_results.extend(results3)
        save_results(results3, "companion_demo3.json")

    # æ€»ç»“
    print("\n" + "=" * 70)
    print("âœ¨ æ¼”ç¤ºå®Œæˆï¼")
    print("=" * 70)
    print()
    print(f"ğŸ“Š æ€»è®¡æå–äº† {len(all_results)} ä¸ªè®°å¿†ç‰‡æ®µ")
    print()
    print("ğŸ“ è¾“å‡ºæ–‡ä»¶:")
    print("   - companion_demo1.json (åŸºç¡€å¯¹è¯)")
    print("   - companion_demo2.json (æ·±åº¦æƒ…æ„Ÿ)")
    print("   - companion_demo3.json (æ··åˆå¯¹è¯)")
    print()
    print("ğŸ¯ å…³é”®ç‰¹æ€§:")
    print("   âœ“ æƒ…æ„Ÿå¯¼å‘è¯„åˆ†")
    print("   âœ“ ä¸ªæ€§åŒ–ä¿¡æ¯ä¼˜å…ˆ")
    print("   âœ“ å…³ç³»æ·±åº¦è€ƒé‡")
    print("   âœ“ ç¨³å®šæ€§ä¿éšœ (æ¸©åº¦0.1 + æœ¬åœ°æ ¡æ­£)")
    print()


if __name__ == "__main__":
    main()
